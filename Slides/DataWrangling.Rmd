---
title: "Data Wrangling"
author: "Ming Li, Amazon </br> [Hui Lin](http://scientistcafe.com), DuPont"
date: "July 30, 2017 @ JSM2017"
output: 
  slidy_presentation: 
    footer: "http://scientistcafe.com"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Issue driven

> - If you don't know where to go ...

> - <img src="images/watch.gif" alt="HTML5 Icon" style="width:300px;height:200px;">

> - If you know where to go ...

> - <img src="images/fun.gif" alt="HTML5 Icon" style="width:300px;height:200px;">


## Outline

- Read and write data
    - `readr`
    - `data.table` — enhanced `data.frame`
- Summarize data
    - `apply()`, `lapply()` and `sapply()` in base R
    - `ddply()` in `plyr` package
    - `dplyr` package
- Tidy and Reshape Data
    - `reshape2` package
    - `tidyr` package

## Read and write data

1. It is 10x faster.
1. It doesn't change the column names.

    ```r
    library(readr)
    read_csv("2015,2016,2017
    1,2,3
    4,5,6")
    ```
    
1. It does not convert strings to factors by default, are able to parse dates and times and can automatically determine the data types in each column. 
1. Killing character : **progress bar**. 

    ![](http://scientistcafe.com/book/Figure/prograssbar.png){width=80%}

## Major functions

- `read_csv()`: reads comma delimited files
- `read_csv2()`: reads semicolon separated files (common in countries where  `,`  is used as the decimal place)
- `read_tsv()`: reads tab delimited files
- `read_delim()`: reads in files with any delimiter
- `read_fwf()`: reads fixed width files. You can specify fields either by their widths with `fwf_widths()`  or their position with  `fwf_positions()`  
- `read_table()`: reads a common variation of fixed width files where columns are separated by white space 
- `read_log()`: reads Apache style log files

## `read_csv()`

```r
library(readr)
sim.dat <- read_csv("https://raw.githubusercontent.com/happyrabbit/DataScientistR/master/Data/SegData.csv ")
head(sim.dat)
```

- `tibble`
    - It never changes an input’s type (i.e., no more `stringsAsFactors = FALSE`!)
    - It never adjusts the names of variables
    - It has a refined print method that shows only the first 10 rows, and all the columns that fit on screen. You can also control the default print behavior by setting options.

Refer to http://r4ds.had.co.nz/tibbles.html for more information about ‘tibble’.

## Baby example 1

```r
dat=read_csv("2015,2016,2017
100,200,300
canola,soybean,corn")
print(dat)
```

## Baby example 2

You can also add comments on the top and tell R to skip those lines:

```r
dat=read_csv("# I will never let you know that
          # my favorite food is carrot
          Date,Food,Mood
          Monday,carrot,happy
          Tuesday,carrot,happy
          Wednesday,carrot,happy
          Thursday,carrot,happy
          Friday,carrot,happy
          Saturday,carrot,extremely happy
          Sunday,carrot,extremely happy", skip = 2)
print(dat)
```

## Baby example 3

- If you don't have column names, set `col_names = FALSE` then R will assign names "`X1`","`X2`"... to the columns:

```r
dat=read_csv("Saturday,carrot,extremely happy
          Sunday,carrot,extremely happy", col_names=FALSE)
print(dat)
```

- You can also pass `col_names`  a character vector which will be used as the column names. Try to replace `col_names=FALSE` with `col_names=c("Date","Food","Mood")` and see what happen.

## `read_csv2()`

- Read semicolon separated files:

```r
dat=read_csv2("Saturday; carrot; extremely happy \n Sunday; carrot; extremely happy", col_names=FALSE)
print(dat)
```

## `read_tsv()`

- Read tab delimited files：

```r
dat=read_tsv("every\tman\tis\ta\tpoet\twhen\the\tis\tin\tlove\n", col_names = FALSE)
print(dat)
```

## `read_delim()`

- Assign separating character

```r
dat=read_delim("THE|UNBEARABLE|RANDOMNESS|OF|LIFE\n", delim = "|", col_names = FALSE)
print(dat)
```

## Substitute for missing value

```r
dat=read_csv("Q1,Q2,Q3
               5, 4,99",na="99")
print(dat)
```

## Write data 

- use `write_csv()` and `write_tsv()`
- Encode strings in UTF-8
- Save dates and date-times in ISO8601 format so they are easily parsed elsewhere


```r
write_csv(sim.dat, "sim_dat.csv")
```

## Other data types

- `Haven`: SPSS, Stata and SAS data
- `Readxl` and `xlsx`: excel data(.xls and .xlsx)
- `DBI`: given data base, such as RMySQL, RSQLite and RPostgreSQL, read data directly from the database using SQL

Useful materials:

- For getting data from internet, you can refere to the book “XML and Web Technologies for Data Sciences with R”. 
- [R data import/export manual](https://cran.r-project.org/doc/manuals/r-release/R-data.html#Acknowledgements)
- `rio` package：https://github.com/leeper/rio

## `data.table`--- enhanced `data.frame`

- What is `data.table`? 
    - It is an R package that provides an enhanced version of `data.frame`. 
    
- Advantages:
    - Offers fast import, subset, grouping, update, and joins for large data files
    - It is easy to turn data frame to data table
    - Can behave just like a data frame

```r
# If you haven't install it, use the code to instal
# install.packages("data.table")
# load packagw
library(data.table)
# covert the existing data frame to data table
dt <- data.table(sim.dat)
class(dt)
```

## Manipulating data table

- Calculate mean for counts of online transactions:

```r
dt[, mean(online_trans)]
```

- You can't do the same thing using data frame:

```r
sim.dat[,mean(online_trans)]
```

## By group

- Calculate mean by group:

```r
dt[ , mean(online_trans), by = gender]
```

- You can group by more than one variables:

```r
dt[ , mean(online_trans), by = .(gender, house)]
```

- Assign column names for aggregated variables:

```r
dt[ , .(avg = mean(online_trans)), by = .(gender, house)]
```

##  General setting of `data.table`

- Different from data frame, there are three arguments for data table:

<center>
![](http://scientistcafe.com/book/Figure/datable1.png)
</center>

- analogous to SQL

<center>
![](http://scientistcafe.com/book/Figure/rSQL.png)
</center>


## Equivalent codes

```r
# Rcode 1
dt[ , mean(online_trans), by = gender]
```

```sql
# SQL 1
SELECT  gender, avg(online_trans) FROM sim.dat GROUP BY gender
```

```r
# Rcode 2
dt[ , .(avg = mean(online_trans)), by = .(gender, house)]
```


```sql 
# SQL 2
SELECT gender, house, avg(online_trans) AS avg FROM sim.dat GROUP BY gender, house
```

```r
# Rcode 3
dt[ age < 40, .(avg = mean(online_trans)), by = .(gender, house)]
```


```sql
# SQL 3
SELECT gender, house, avg(online_trans) AS avg FROM sim.dat WHERE age < 40 GROUP BY gender, house
```

## Subset

- select row

```r
# select rows with age<20 and income > 80000
dt[age < 20 & income > 80000]
# select the first two rows
dt[1:2]
```
- select column

```r
# select column “age” but return it as a vector
# the argument for row is empty so the result will return all observations
ans <- dt[, age]
head(ans)
# To return `data.table` object, put column names in `list()`
# Select age and online_exp columns and return as a data.table instead
ans <- dt[, list(age, online_exp)]
head(ans)
# you can also put column names in `.()`
ans <- dt[, .(age, online_exp)]
# head(ans)
# To select all columns from “age” to “income”
ans <- dt[, age:income, with = FALSE]
head(ans,2)
# Delete columns using `-` or `!`
# delete columns from  age to online_exp
ans <- dt[, -(age:online_exp), with = FALSE]
ans <- dt[, !(age:online_exp), with = FALSE]
```

## tabulation

- In data table. `.N` means to count。

```r
# row count
dt[, .N] 
```

- Count by groups:

```r
# counts by gender
dt[, .N, by= gender]  
# for those younger than 30, count by gender
 dt[age < 30, .(count=.N), by= gender] 
```

- Order table:

```r
# get records with the highest 5 online expense:
head(dt[order(-online_exp)],5) 
# Since data table keep some characters of data frame, they share some operations
dt[order(-online_exp)][1:5]
# order the table by more than one variable
dt[order(gender, -online_exp)][1:5]
```

## Use `fread()` to import dat

```r
system.time(topic<-read.csv("https://raw.githubusercontent.com/happyrabbit/DataScientistR/master/Data/topic.csv"))
```

```html
  user  system elapsed 
  4.313   0.027   4.340
```

```r
system.time(topic<-readr::read_csv("https://raw.githubusercontent.com/happyrabbit/DataScientistR/master/Data/topic.csv"))
```

```html
   user  system elapsed 
  0.267   0.008   0.274 
```

```r
system.time(topic<-data.table::fread("https://raw.githubusercontent.com/happyrabbit/DataScientistR/master/Data/topic.csv"))
```

```html
   user  system elapsed 
  0.217   0.005   0.221 
```

## `apply()`, `lapply()` and `sapply()` in base R

- Similarity: apply functions over parts of data
- Difference:
    - the type of object they apply to
    - the type of result they will return
    
## When do we use `apply()`?

- apply a function to margins of an array or matrix
- returns a vector or array or list of values

```r
## simulate a matrix
x <- cbind(x1 =1:8, x2 = c(4:1, 2:5))
dimnames(x)[[1]] <- letters[1:8]
apply(x, 2, mean)
col.sums <- apply(x, 2, sum)
row.sums <- apply(x, 1, sum)
```

- apply other functions:

```r
ma <- matrix(c(1:4, 1, 6:8), nrow = 2)
ma
apply(ma, 1, table)  #--> a list of length 2
apply(ma, 1, stats::quantile) # 5 x n matrix with rownames
```

## Tricky example

- Results can have different lengths for each call. 
- This is a trickier example. What will you get? 

```r
## Example with different lengths for each call
z <- array(1:24, dim = 2:4)
zseq <- apply(z, 1:2, function(x) seq_len(max(x)))
zseq         ## a 2 x 3 matrix
typeof(zseq) ## list
dim(zseq) ## 2 3
zseq[1,]
apply(z, 3, function(x) seq_len(max(x)))
```

## `lapply()` and `sapply()`

- `lapply()` applies a function over a list, data.frame or vector and returns a list of the same length.
- `sapply()` is a user-friendly version and wrapper of `lapply()`. By default it returns a vector, matrix or if  `simplify = "array"`, an array if appropriate. `apply(x, f, simplify = FALSE, USE.NAMES = FALSE)` is the same as `lapply(x, f)`. If `simplify=TRUE`, then it will return a `data.frame` instead of `list`. 

## Example using data with context

- Get the mean and standard deviation of all numerical variables in the data set.

```r
# Read data
sim.dat<-read.csv("https://raw.githubusercontent.com/happyrabbit/DataScientistR/master/Data/SegData.csv")
# Get numerical variables
sdat<-sim.dat[,!lapply(sim.dat,class)=="factor"]
## Try the following code with apply() function
## apply(sim.dat,2,class)
## What is the problem?
```

- use `apply()` to get mean and standard deviation for each column

```r
apply(sdat, MARGIN=2,function(x) mean(na.omit(x)))
apply(sdat, MARGIN=2,function(x) sd(na.omit(x)))
```

- Run the following code and compare the results:

```r
lapply(sdat, function(x) sd(na.omit(x)))
sapply(sdat, function(x) sd(na.omit(x)))
sapply(sdat, function(x) sd(na.omit(x)), simplify = FALSE)
```

## `dplyr` package

- Next iteration of `plyr` package
- Flexible grammar of data manipulation focusing on tools for working with data frames (hence the `d` in the name)
- It identifies the most important data manipulations and make they easy to use from R
- It performs faster for in-memory data by writing key pieces in C++ using `Rcpp`

## `dplyr` package

1. Display
1. Subset
1. Summarize
1. Create new variable
1. Merge

## Display

- `tbl_df()`: Convert the data to `tibble` 

```r
library(dplyr)
tbl_df(sim.dat)
```

- `glimpse()`: This is like a transposed version of `tbl_df()`

```r
glimpse(sim.dat)
```

## Subset

- Get rows with `income` more than 300000:

```r
filter(sim.dat, income >300000) %>%
  tbl_df()
```

- Here we meet a new operator `%>%`: "Pipe operator" 

## Pipe operator: `%>%`

- It pipes a value forward into an expression or function call
- What you get in the left operation will be the first argument or the only argument in the right operation.

```r
x %>% f(y) = f(x, y)
y %>% f(x, ., z) = f(x, y, z )
```

For example: `"Hello World" %>% substring(7, 11) %>% grepl("Wo", .)`

## Pipe operator: `%>%`

Look at the following code. Can you tell me what it does?

```r
ave_exp <- filter( 
  summarise(
    group_by( 
      filter(
        sim.dat, 
        !is.na(income)
      ), 
      segment
    ), 
    ave_online_exp = mean(online_exp), 
    n = n()
  ), 
  n > 200
) 
```
Now look at the identical code using "`%>%`":

```r
avg_exp <- sim.dat %>% 
 filter(!is.na(income)) %>% 
 group_by(segment) %>% 
 summarise( 
   ave_online_exp = mean(online_exp), 
   n = n() ) %>% 
  filter(n > 200)
```

## Let's read it

```r
avg_exp <- sim.dat %>% 
 filter(!is.na(income)) %>% 
 group_by(segment) %>% 
 summarise( 
   ave_online_exp = mean(online_exp), 
   n = n() ) %>% 
  filter(n > 200)
```

## Subset - select rows
 
- `distinct()`: a generalization of `unique()` from vector to data frame

```r
dplyr::distinct(sim.dat)
```

- `sample_frac()`: randomly select some rows with specified percentage. 
- `sample_n()`:randomly select rows with specified number.

```r
dplyr::sample_frac(sim.dat, 0.5, replace = TRUE) 
dplyr::sample_n(sim.dat, 10, replace = TRUE) 
```

- `slice()` will select rows by position:

```r
# It is equivalent to `sim.dat[10:15,]`
dplyr::slice(sim.dat, 10:15) 
```

- `top_n()`  select the order top n entries:

```r
dplyr::top_n(sim.dat,2,income)
```

## Subset - select columns

```r
# select by column name
dplyr::select(sim.dat,income,age,store_exp)

# select columns whose name contains a character string
dplyr::select(sim.dat, contains("_"))

# select columns whose name ends with a character string
# similar there is "starts_with"
dplyr::select(sim.dat, ends_with("e"))

# select columns Q1,Q2,Q3,Q4 and Q5
select(sim.dat, num_range("Q", 1:5)) 

# select columns whose names are in a group of names
dplyr::select(sim.dat, one_of(c("age", "income")))

# select columns between age and online_exp
dplyr::select(sim.dat, age:online_exp)

# select all columns except for age
dplyr::select(sim.dat, -age)
```

## Summarize

```r
dplyr::summarise(sim.dat, avg_online = mean(online_trans)) 
# apply function anyNA() to each column
# you can also assign a function vector such as: c("anyNA","is.factor")
dplyr::summarise_each(sim.dat, funs_(c("anyNA")))
```

- `group_by()` 

```r
sim.dat %>% group_by(segment) %>% summarise_each(funs_(c("anyNA")))
```

## Create new variable

- `mutate()`: compute and append one or more new columns:

```r
dplyr::mutate(sim.dat, total_exp = store_exp + online_exp)
```

- It will apply **window function** to the columns and return a column with the same length

```r
# min_rank=rank(ties.method = "min")
# mutate_each() means apply function to each column
dplyr::mutate_each(sim.dat, funs(min_rank)) 
```

-  `transmute()`: delete the original columns and only keep the new ones

```r
dplyr::transmute(sim.dat, total_exp = store_exp + online_exp) 
```

## Merge

```r
(x<-data.frame(cbind(ID=c("A","B","C"),x1=c(1,2,3))))
(y<-data.frame(cbind(ID=c("B","C","D"),y1=c(T,T,F))))
```

```r
# join to the left
# keep all rows in x
left_join(x,y,by="ID")
# get rows matched in both data sets
inner_join(x,y,by="ID")
# get rows in either data set
full_join(x,y,by="ID")
# filter out rows in x that can be matched in y 
# it doesn't bring in any values from y 
semi_join(x,y,by="ID")
# the opposite of  semi_join()
# it gets rows in x that cannot be matched in y
# it doesn't bring in any values from y
anti_join(x,y,by="ID")
```

## Tidy and Reshape Data 


-  "Tidy data" represent the information from a dataset as data frames where each row is an observation and each column contains the values of a variable
- convert data between the "wide" and the "long" format
- two commonly used packages for this kind of manipulations: `tidyr` and `reshape2`

## `reshape2` package

- reboot of previous package `reshape`
- main functions:
    1. `melt()` to convert an object into a molten data frame, i.e. from wide to long
    1. `dcast()` to cast a molten data frame into the shape you want, i.e. from long to wide

```r
# Take a baby subset of our exemplary clothes consumers data to illustrate:
(sdat<-sim.dat[1:5,1:6])
```

## `reshape2` example

- have a variable indicating the purchasing channel (i.e. online or in-store) and another column with the corresponding expense amount

```r
library(reshape2)
(mdat <- melt(sdat, measure.vars=c("store_exp","online_exp"),
              variable.name = "Channel",
              value.name = "Expense"))
```


- You can run a regression to study the effect of purchasing channel: 

```r
# Here we use all observations from sim.dat
mdat<-melt(sim.dat[,1:6], measure.vars=c("store_exp","online_exp"),
            variable.name = "Channel",
              value.name = "Expense")
fit<-lm(Expense~gender+house+income+Channel+age,data=mdat)
summary(fit)
```

## `reshape2` example

- compare the online and in store expense between male and female based on the house ownership

```r
dcast(mdat, house + gender ~ Channel, sum)
```

- left side : variables that you want to group by
- right side: variable you want to spread as columns

## `tidyr` package

- Get a baby set to illustrate:

```r
library(dplyr)
library(tidyr)
# practice functions we learnt before
sdat<-sim.dat[1:5,]%>%
  dplyr::select(age,gender,store_exp,store_trans)
sdat %>% tbl_df()
```

## `gather()`

- Analogous to `melt()` in `reshape2`

```r
library(tidyr)
msdat<-tidyr::gather(sdat,"variable","value",store_exp,store_trans)
msdat %>% tbl_df()
```

-  if we use the pipe operation,

```r
sdat%>%gather("variable","value",store_exp,store_trans)
```

- It is identical with the following code using `melt()`:

```r
library(reshape2)
melt(sdat, measure.vars=c("store_exp","store_trans"),
            variable.name = "variable",
              value.name = "value")
```

## `spread()`


```r
msdat %>% spread(variable,value)
```

##  `separate()` and `unite()`

```r
# You can use `sep=` 
# By default, it is "`_`"
sepdat<- msdat %>% 
  separate(variable,c("Source","Type"))
sepdat %>% tbl_df()
```

```r
sepdat %>% 
  unite("variable",Source,Type,sep="_")
```

## Some links

- SQL Learning: https://www.w3schools.com/sql/

